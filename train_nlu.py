import argparse
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)
import math
from evaluate import load as load_metric
import wandb
import os
import json
import tempfile
from peft import get_peft_model, LoraConfig, AdaLoraConfig
from peft.tuners.lava.config import LavaConfig

from configs.task_config import (
    GLUE_META,
    PISSA_TASK_CONFIG,
    DORA_TASK_CONFIG,
    LORA_TASK_CONFIG,
    MOCA_TASK_CONFIG,
    LAVA_TASK_CONFIG,
    BITFIT_TASK_CONFIG,
    ADALORA_TASK_CONFIG,
)

from trainer import (
    LavaNLUTrainer,
    setup_seed,
    register_lava,
    BestMetricCallback,
    print_trainable_parameters,
)

# LAVA ë“±ë¡
register_lava()

# ==========================================================
# MaxEnt LAVA Trainer (ğŸ”¥ CLEAN: FIXED LAMBDA, NO CONSTRAINT)
# ==========================================================

# ==========================================================
# Adapter builder (MODIFIED: Added AdaLoRA, BitFit)
# ==========================================================
def build_adapter(adapter_type, r, alpha, model=None):
    at = adapter_type.lower()

    # 1. LoRA ê³„ì—´ (LoRA, DoRA, PiSSA)
    if at in ["lora", "dora", "pissa"]:
        kwargs = dict(
            r=r,
            lora_alpha=alpha,
            target_modules=["query_proj", "key_proj", "value_proj", "dense"],
            task_type="SEQ_CLS",
        )
        if at == "pissa":
            kwargs["init_lora_weights"] = "pissa"
        if at == "dora":
            kwargs["use_dora"] = True
        return LoraConfig(**kwargs)

    # 2. AdaLoRA
    if at == "adalora":
        return AdaLoraConfig(
            r=r,
            lora_alpha=alpha,
            target_modules=["query_proj", "key_proj", "value_proj", "dense"],
            task_type="SEQ_CLS",
        )

    # 3. LAVA
    if at in ["lava", "lava_init"]:
        return LavaConfig(
            r=r,
            alpha=alpha,
            target_modules=["query_proj", "key_proj", "value_proj", "dense"],
            task_type="SEQ_CLS",
        )

    # 4. BitFit
    if at == "bitfit":
        return "bitfit"

    raise ValueError(f"Unknown adapter type: {adapter_type}")


# ==========================================================
# MAIN
# ==========================================================
def main(args):
    task = args.task
    adapter_type = args.adapter
    
    meta = GLUE_META[task]
    num_labels = meta["num_labels"]
    main_metric = meta["main"]
    eval_key = meta["eval_key"]
    
    raw = load_dataset("glue", task)
    tokenizer = AutoTokenizer.from_pretrained(args.model)

    def preprocess(batch):
        return tokenizer(
            batch[meta["s1"]],
            batch[meta["s2"]] if meta["s2"] else None,
            truncation=True,
            padding="max_length",
            max_length=128,
        )

    encoded = raw.map(
        preprocess,
        batched=True,
        keep_in_memory=True,  # ë””ìŠ¤í¬ ìºì‹œ ìƒì„± ë°©ì§€
        load_from_cache_file=False  # ê¸°ì¡´ ìºì‹œ ë¬´ì‹œ
    )
    encoded = encoded.rename_column("label", "labels")
    encoded.set_format(
        type="torch",
        columns=["input_ids", "attention_mask", "labels"],
    )

    base = AutoModelForSequenceClassification.from_pretrained(
        args.model,
        num_labels=num_labels,
    )

    # Config ë§¤í•‘
    at = adapter_type.lower()
    config_map = {
        "pissa": PISSA_TASK_CONFIG,
        "dora": DORA_TASK_CONFIG,
        "lora": LORA_TASK_CONFIG,
        "lava": LAVA_TASK_CONFIG,
        "adalora": ADALORA_TASK_CONFIG,
        "bitfit": BITFIT_TASK_CONFIG
    }
    
    task_configs = config_map.get(at, LORA_TASK_CONFIG)
    cfg = task_configs.get(task, task_configs.get("default", {"epochs": 3, "lr": 2e-4, "batch": 32}))

    epochs = args.epochs if args.epochs is not None else cfg["epochs"]
    lr = args.learning_rate if args.learning_rate is not None else cfg.get("lr", 5e-4)
    batch = args.batch if args.batch is not None else cfg.get("batch", 32)
    alpha = args.alpha if args.alpha is not None else cfg.get("alpha", args.r * 2)

    if args.alpha is not None:
        # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ --alphaë¥¼ ì¤€ ê²½ìš°
        final_alpha = args.alpha
    elif at in ["lava", "lava_init"]:
        # LAVAì¼ ê²½ìš° r=8, alpha=4 ê¸°ì¤€ sqrt scaling ì ìš©
        # ê³µì‹: alpha = 4 * sqrt(r / 8)
        final_alpha = 4 * math.sqrt(args.r / 8)
        print(f"[*] LAVA Optimal Alpha calculated: {final_alpha:.2f} (r={args.r})")
    else:
        # ì¼ë°˜ LoRA ê³„ì—´ì€ ê¸°ì¡´ ë°©ì‹(cfg ë˜ëŠ” r*2) ìœ ì§€
        final_alpha = cfg.get("alpha", args.r)


    # Adapter ì ìš©
    peft_cfg = build_adapter(adapter_type, r=args.r, alpha=final_alpha)
    
    
    
        
    if at == "pissa":
        cache_dir = ".precomputed"
        os.makedirs(cache_dir, exist_ok=True)
        
        # ëª¨ë¸ëª…ê³¼ Rankë¥¼ ì¡°í•©í•´ ê³ ìœ  íŒŒì¼ëª… ìƒì„±
        model_name_safe = args.model.replace("/", "_")
        cache_path = os.path.join(cache_dir, f"{model_name_safe}_r{args.r}.pt")

        if os.path.exists(cache_path):
            print(f"[*] Found precomputed PiSSA weights at {cache_path}. Loading...")
            # ìºì‹œê°€ ìˆìœ¼ë©´ SVD ì—°ì‚°ì„ ê±´ë„ˆëœ€
            peft_cfg.init_lora_weights = False
            model = get_peft_model(base, peft_cfg)
            
            # ì €ì¥ëœ PiSSA ê°€ì¤‘ì¹˜(A, B ë° ìˆ˜ì •ëœ base) ì£¼ì…
            checkpoint = torch.load(cache_path, map_location="cpu")
            model.load_state_dict(checkpoint, strict=False)
            print(f"[*] PiSSA initialization skipped and weights loaded from cache.")
        else:
            print(f"[*] No precomputed weights found. Computing PiSSA SVD (this may take a while)...")
            peft_cfg.init_lora_weights = "pissa"
            model = get_peft_model(base, peft_cfg)
            
            # ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ ì €ì¥
            to_save = {}
            for name, param in model.named_parameters():
                if "lora_" in name or "pissa" in name or any(tm in name for tm in peft_cfg.target_modules):
                    if param.requires_grad or "base_layer" in name:
                        to_save[name] = param.cpu().detach()
            
            torch.save(to_save, cache_path)
            print(f"[*] PiSSA SVD computation finished and saved to {cache_path}")

    elif adapter_type.lower() == "bitfit":
        # BitFit ì „ìš© ë¡œì§
        model = base
        for name, param in model.named_parameters():
            if "bias" in name or "classifier" in name or "pooler" in name:
                param.requires_grad = True
            else:
                param.requires_grad = False
        print("[*] BitFit adapter applied.")

    else:
        # LAVA, LoRA, DoRA ë“± ì¼ë°˜ì ì¸ ì–´ëŒ‘í„° ìƒì„±
        model = get_peft_model(base, peft_cfg)
        print(f"[*] {adapter_type.upper()} adapter applied.") 
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())
    trainable_percentage = 100 * trainable_params / all_params   
    print(f"trainable params: {trainable_params:,} || all params: {all_params:,} || trainable%: {trainable_percentage:.4f}")
    
    metric = load_metric("glue", task)
    

    def compute_metrics(eval_pred):
        preds, labels = eval_pred
        if num_labels == 1:
            preds = preds.squeeze()
        else:
            preds = preds.argmax(-1)
        
        out = metric.compute(predictions=preds, references=labels)
        out["main"] = out[main_metric]
        return out
    
    
    run_name = (
        f"{adapter_type}_{task}_"
        f"r{args.r}_a{final_alpha:.1f}_"
        f"vb{args.lambda_vib}_st{args.lambda_stab}_"
        f"ls{args.lambda_latent_stability}_"
        f"s{args.seed}"
    )
    
    wandb.init(
        project="Deberta-NaturalLanguageUnderstanding",
        name=run_name,
        config=vars(args)
    )
    wandb.run.summary["trainable_params"] = trainable_params
    wandb.run.summary["all_params"] = all_params
    wandb.run.summary["trainable_percentage"] = trainable_percentage

    best_callback = BestMetricCallback(main_metric)

    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš© (í´ë” ìƒì„± ë°©ì§€)
    tmp_dir = tempfile.mkdtemp()

    args_out = TrainingArguments(
        output_dir=tmp_dir,
        evaluation_strategy="epoch",
        save_strategy="no",
        learning_rate=lr,
        per_device_train_batch_size=batch,
        per_device_eval_batch_size=batch,
        num_train_epochs=epochs,
        weight_decay=args.weight_decay,
        warmup_ratio=args.warmup_ratio,
        lr_scheduler_type=args.lr_scheduler,
        max_grad_norm=args.max_grad_norm,
        report_to="wandb",
        seed=args.seed,
        logging_steps=10,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False}
    )

    if at in ["lava", "lava_init"]:
        trainer = LavaNLUTrainer(
            model=model,
            args=args_out,
            train_dataset=encoded["train"],
            eval_dataset=encoded[eval_key],
            compute_metrics=compute_metrics,
            tokenizer=tokenizer,
            callbacks=[best_callback],
            lambda_vib=args.lambda_vib,
            lambda_stab=args.lambda_stab,
            lambda_latent_stability=args.lambda_latent_stability,
        )
    else:
        trainer = Trainer(
            model=model,
            args=args_out,
            train_dataset=encoded["train"],
            eval_dataset=encoded[eval_key],
            tokenizer=tokenizer,
            compute_metrics=compute_metrics,
            callbacks=[best_callback],
        )

    trainer.train()
    best_main = None
    for log in trainer.state.log_history:
        if f"eval_{main_metric}" in log:
            val = log[f"eval_{main_metric}"]
            best_main = val if best_main is None else max(best_main, val)

    if best_main is not None:
        wandb.run.summary[f"best_{main_metric}"] = best_main

    wandb.finish()

    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë¯¼ê°ë„ ë¶„ì„ìš©)
    result_dir = os.path.join(os.path.dirname(__file__), "results")
    os.makedirs(result_dir, exist_ok=True)

    result_file = os.path.join(
        result_dir,
        f"result_{task}_s{args.seed}_vib{args.lambda_vib}_stab{args.lambda_stab}_lat{args.lambda_latent_stability}.json"
    )

    result_data = {
        "task": task,
        "seed": args.seed,
        "lambda_vib": args.lambda_vib,
        "lambda_stab": args.lambda_stab,
        "lambda_latent_stability": args.lambda_latent_stability,
        "best_metric": best_main if best_main is not None else 0.0,
        "metric_name": main_metric
    }

    with open(result_file, "w") as f:
        json.dump(result_data, f, indent=2)

    print(f"\n[RESULT] Task: {task}, Best {main_metric}: {best_main:.4f}" if best_main else f"\n[RESULT] Task: {task}, No valid metric")

    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
    import shutil
    if os.path.exists(tmp_dir):
        shutil.rmtree(tmp_dir)

    return best_main


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # Task & Model
    parser.add_argument("--task", type=str, required=True)
    parser.add_argument("--adapter", type=str, required=True)
    parser.add_argument("--model", type=str, default="microsoft/deberta-v3-base")

    # General Training Parameters
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--learning_rate", type=float, default=5e-4)
    parser.add_argument("--batch", type=int, default=32)
    parser.add_argument("--epochs", type=int, default=None, help="Override epochs from config")
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--warmup_ratio", type=float, default=0.1)
    parser.add_argument("--lr_scheduler", type=str, default="linear",
                        choices=["linear", "cosine", "constant"])
    parser.add_argument("--max_grad_norm", type=float, default=1.0)

    # LoRA Parameters
    parser.add_argument("--r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--alpha", type=int, default=None, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.1)

    # LAVA Specific Parameters
    parser.add_argument("--lambda_vib", type=float, default=1.0, help="VIB loss weight")
    parser.add_argument("--lambda_stab", type=float, default=0.1, help="Logit stability weight")
    parser.add_argument("--lambda_latent_stability", type=float, default=1.0, help="Latent stability weight")
    parser.add_argument("--latent_dim", type=int, default=16, help="LAVA latent dimension")
    parser.add_argument("--kl_annealing", action="store_true", help="Enable KL annealing")
    parser.add_argument("--noise_scale", type=float, default=1.0, help="LAVA noise scale")

    args = parser.parse_args()
    setup_seed(args.seed)
    main(args)